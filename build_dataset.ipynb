{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the dataset from the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import nibabel as nib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "np.random.seed(1312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nii(filepath):\n",
    "    '''\n",
    "    Reads .nii file and returns pixel array\n",
    "    '''\n",
    "    ct_scan = nib.load(filepath)\n",
    "    array   = ct_scan.get_fdata()\n",
    "    array   = np.rot90(np.array(array))\n",
    "    return(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all slides to npy files\n",
    "GENERATE_NPY_FILES = False\n",
    "\n",
    "\n",
    "if (GENERATE_NPY_FILES) :\n",
    "\n",
    "    archive_path_part1 =  \"./data_seg_part1.zip\"\n",
    "    archive_path_part2 =  \"./data_seg_part2.zip\"\n",
    "    export_path_ct = \"./data/ct\"\n",
    "    export_path_mask = \"./data/mask\"\n",
    "    os.makedirs(export_path, exist_ok=True)\n",
    "    os.makedirs(export_path_ct, exist_ok=True)\n",
    "    os.makedirs(export_path_mask, exist_ok=True)\n",
    "    \n",
    "\n",
    "    # opening the zip file in READ mode\n",
    "    with ZipFile(archive_path_part1, 'r') as zip:\n",
    "        # extracting all the files\n",
    "        print('Extracting all the files now...')\n",
    "        zip.extractall(\"./whole_data/\") # This folder will be removed after the whole process\n",
    "        print('Done!')\n",
    "\n",
    "    # with ZipFile(archive_path_part2, 'r') as zip:\n",
    "    #     # extracting all the files\n",
    "    #     print('Extracting all the files now...')\n",
    "    #     zip.extractall(\"./whole_data/\")\n",
    "    #     print('Done!')\n",
    "\n",
    "\n",
    "    # Create a meta file for nii files processing\n",
    "    file_list = []\n",
    "    for dirname, _, filenames in os.walk('./whole_data/'):\n",
    "        for filename in filenames:\n",
    "            file_list.append((dirname, filename)) \n",
    "\n",
    "\n",
    "    df_files = pd.DataFrame(file_list, columns =['dirname', 'filename']) \n",
    "    df_files.sort_values(by=['filename'], ascending=True)  \n",
    "\n",
    "\n",
    "    # Map CT scan and label \n",
    "\n",
    "    df_files[\"mask_dirname\"]  = \"\"\n",
    "    df_files[\"mask_filename\"] = \"\"\n",
    "\n",
    "    for i in range(131):\n",
    "        ct = f\"volume-{i}.nii\"\n",
    "        mask = f\"segmentation-{i}.nii\"\n",
    "        \n",
    "        df_files.loc[df_files['filename'] == ct, 'mask_filename'] = mask\n",
    "        df_files.loc[df_files['filename'] == ct, 'mask_dirname'] = \"./whole_data/segmentations\"\n",
    "\n",
    "    # drop segment rows\n",
    "    df_files = df_files[df_files.mask_filename != ''].sort_values(by=['filename']).reset_index(drop=True) \n",
    "\n",
    "    whole_data_df = pd.DataFrame(columns=['patient_id', 'slice_id', 'ct_path', 'mask_path'])\n",
    "    print(\"Exporting all slices to npy files...\")\n",
    "    for ii in tqdm(range(len(df_files))):\n",
    "        row = df_files.iloc[ii]\n",
    "        #Retrieve patient ID\n",
    "        patient_id = row['filename'].split('-')[1][:-4]\n",
    "\n",
    "        #Load scan and mask\n",
    "        scan = read_nii(row['dirname']+\"/\"+row['filename'])\n",
    "        mask = read_nii(row['mask_dirname']+\"/\"+row['mask_filename'])\n",
    "\n",
    "        #Save each slice as a npy file\n",
    "        for slice_id in range(scan.shape[2]):\n",
    "            ct_path = f'{export_path_ct}/patient_{patient_id}_slice_{slice_id}_scan.npy'\n",
    "            mask_path = f'{export_path_mask}/patient_{patient_id}_slice_{slice_id}_mask.npy'\n",
    "            np.save(ct_path, scan[:,:,slice_id])\n",
    "            np.save(mask_path, mask[:,:,slice_id])\n",
    "\n",
    "            whole_data_df = pd.concat([whole_data_df, \n",
    "                                       pd.DataFrame([[patient_id, slice_id, ct_path, mask_path]],\n",
    "                                            columns = ['patient_id', 'slice_id', 'ct_path', 'mask_path'])],\n",
    "                                      ignore_index=True)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    whole_data_df.to_csv(f'{export_path}/whole_data_df.csv', index=False)\n",
    "\n",
    "    # Remove whole_data folder\n",
    "    shutil.rmtree('./whole_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test patients:  [43 39  8 38 13  1  2 21 31 33]\n",
      "Val patients:  [10  0 50  7 28]\n",
      "Train patients:  [ 3  4  5  6  9 11 12 14 15 16 17 18 19 20 22 23 24 25 26 27 29 30 32 34\n",
      " 35 36 37 40 41 42 44 45 46 47 48 49]\n"
     ]
    }
   ],
   "source": [
    "BUILD_SPLIT_DATASET = False\n",
    "\n",
    "if (BUILD_SPLIT_DATASET) :\n",
    "    test_frac = 0.2\n",
    "    val_frac = 0.1\n",
    "\n",
    "    patient_id = whole_data_df['patient_id'].unique()\n",
    "    n_patient = len(patient_id)\n",
    "\n",
    "    n_test = int(n_patient * test_frac)\n",
    "    n_val = int(n_patient * val_frac)\n",
    "    n_train = n_patient - n_test - n_val\n",
    "\n",
    "    # Randomly select test/train patients\n",
    "    test_patients = np.random.choice(patient_id, n_test, replace=False)\n",
    "    val_patients = np.random.choice(np.setdiff1d(patient_id, test_patients), n_val, replace=False)\n",
    "    train_patients = np.setdiff1d(patient_id, np.concatenate([test_patients, val_patients]))\n",
    "\n",
    "    print(\"Test patients: \", test_patients)\n",
    "    print(\"Val patients: \", val_patients)\n",
    "    print(\"Train patients: \", train_patients)\n",
    "\n",
    "    # Split dataset\n",
    "    train_df = whole_data_df[whole_data_df['patient_id'].isin(train_patients)]\n",
    "    val_df = whole_data_df[whole_data_df['patient_id'].isin(val_patients)]\n",
    "    test_df = whole_data_df[whole_data_df['patient_id'].isin(test_patients)]\n",
    "\n",
    "    # Save train/test dataset\n",
    "    train_df.to_csv(f'{export_path}/train_df.csv', index=False)\n",
    "    val_df.to_csv(f'{export_path}/val_df.csv', index=False)\n",
    "    test_df.to_csv(f'{export_path}/test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13996/13996 [01:21<00:00, 171.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT mean:  -678.6109391237196\n",
      "CT std:  656.9885500358824\n",
      "CT max:  3071.0\n",
      "CT min:  -3024.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "COMPUTE_MEAN_STD = False\n",
    "\n",
    "if (COMPUTE_MEAN_STD) :\n",
    "    # Compute mean and std of train dataset\n",
    "    train_df = pd.read_csv(f'{export_path}/train_df.csv')\n",
    "    train_ct_paths = train_df['ct_path'].values\n",
    "\n",
    "    ct = np.load(train_ct_paths[0])\n",
    "\n",
    "    ct_mean = np.mean(ct)\n",
    "    ct_std = np.std(ct)\n",
    "    ct_max = np.max(ct)\n",
    "    ct_min = np.min(ct)\n",
    "\n",
    "\n",
    "    for i in tqdm(range(1, len(train_ct_paths))):\n",
    "        ct = np.load(train_ct_paths[i])\n",
    "\n",
    "        ct_mean += np.mean(ct)\n",
    "        ct_std += np.std(ct)\n",
    "        ct_max = max(ct_max, np.max(ct))\n",
    "        ct_min = min(ct_min, np.min(ct))\n",
    "\n",
    "\n",
    "    ct_mean /= len(train_ct_paths)\n",
    "    ct_std /= len(train_ct_paths)\n",
    "\n",
    "    print(\"CT mean: \", ct_mean)\n",
    "    print(\"CT std: \", ct_std)\n",
    "    print(\"CT max: \", ct_max)\n",
    "    print(\"CT min: \", ct_min)\n",
    "\n",
    "    # Save mean and std\n",
    "    np.save(f'{export_path}/ct_mean.npy', ct_mean)\n",
    "    np.save(f'{export_path}/ct_std.npy', ct_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
